#id iteration crawler 

import itertool 

def crawl_site(url):
  for page in itertools.count(1) 
  pg_url = '{}{}'.format(url,page) 
  html= download(pg_url) 
  if html is None: 
    break 
 #sucess - can scrap results 
 
 def crawl_site(url, max_errors = 5) 
    for page in itertools.count(1): 
    pd_url = '{}{}'.format(url, page) 
    if html is None: 
      num_errors += 1 
      if num_errors == max_errors: 
        #Max errors reach exit loop 
        break 
      else: 
        num_errors = 0 
      #sucess - can scrap the results 
      
